import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# Loaded environment variables from .env file
load_dotenv()

# Initialize the Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')  

def read_and_chunk_files(folder_path):
    chunks = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as f:
                text = f.read()
                sentences = text.split('\n')
                for i in range(0, len(sentences), 2):
                    chunk = " ".join(sentences[i:i+2])
                    chunks.append(chunk)
    return chunks

def create_vector_index(chunks):
    embeddings = model.encode(chunks)  
    embeddings = np.array(embeddings).astype('float32')
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index, embeddings

def search_query(query, index, chunks, top_k=3):
    query_embedding = model.encode([query])
    query_embedding = np.array(query_embedding).astype('float32')
    distances, indices = index.search(query_embedding, top_k)
    results = []
    for i in range(top_k):
        idx = indices[0][i]
        result = {
            'chunk': chunks[idx],
            'distance': distances[0][i]
        }
        results.append(result)
    return results

def generate_answer_from_llm(retrieved_chunks, query=""):
    context = "\n".join([result['chunk'] for result in retrieved_chunks])
    system_prompt = "You are a helpful assistant that answers questions based on the provided context."
    user_prompt = f"Context:\n{context}\n\nQuestion: {query}\nAnswer:"

    from transformers import pipeline
    generator = pipeline("text-generation", model="gpt2")
    answer = generator(user_prompt, max_length=150)[0]['generated_text']
    
    return answer

if __name__ == "__main__":
    folder_path = "docs"
    chunks = read_and_chunk_files(folder_path)
    print(f"Total Chunks: {len(chunks)}")

    index, embeddings = create_vector_index(chunks)
    
    query = "What is the return policy?"
    retrieved_chunks = search_query(query, index, chunks, top_k=3)

    print("\nTop 3 Retrieved Chunks:")
    for i, result in enumerate(retrieved_chunks):
        print(f"Result {i+1}: {result['chunk']}")
    
    answer = generate_answer_from_llm(retrieved_chunks, query)
    print("\nFinal Answer Generated by LLM:")
    print(answer)




